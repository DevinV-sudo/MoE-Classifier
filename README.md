# MoE-Classifier: MoE-Gating for RAG
Infers routing decisions based on synthetic user prompts that are agentically generated by clustering each RAG-Experts knowledge base.
---

### Project Overview
This project is a component of a larger system I have been devloping to provide intelligent, course-specific responses, tailored to students by leveraging their course data, LLMS, and Deep learning.
The model architecture in broad strokes is a combination of MoE and Retreival Augmentaion where each expert is a RAG system whose knowledge base is composed of transformed multi-modal class material for a specific course.
This repository documents the development of the MoE-Classifier, a gating mechanism that routes student queries to the expert model trained on the relevant course data for their inquiry.

### Purpose
Several key challenges drove the architecture architectural decisions for this model:
1. As more knowledge bases and experts are added, the classification task grows increasingly complex.
2. The gating mechanism needs to be fast during inference to avoid bottlenecking the expert's response.
3. The system needs to scale efficiently to handle queries of varying complexity.

In response to these challenges, I decided that an "adaptive capacity" approach was needed. I chose DistilBERT as the backbone for the classifier to leverage its pretrained transformer architecture for sequence classification. 
My initial strategy involved extracting hidden states and applying an MLP to predict, on an input-by-input basis, whether to pass the hidden state’s CLS token to the classifier head or keep it within the transformer block.
This method was partially successful but lended itself to "hard-code" exits at either extreme, and additionally the input-to-input approach was quite complex to implement and evaluate.  

In response to these challenges, I shifted to the conceptual architecture presented in:  
[DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference](https://arxiv.org/abs/2109.11745)  
The architecture is similar to my initial approach, but adds a cumulative representation of the CLS token as the output, and cumulate exit confidence while operating in a batch-by-batch fasion.
 
   
### Data Generation
The dataset for this classifier consists of synthetic user queries. These queries are generated by first applying PCA to transform the vector embeddings of each RAG expert’s knowledge base. The transformed embeddings are then clustered using HDBSCAN, after which they are reverse-transformed. The extracted test metadata from the embeddings from each cluster is concatenated and batched to use as context. GPT-3.5 takes this context and is prompted to generate probable queries relating to the context generated by each cluster. In theory, since these clusters group text embeddings with high cosine similarity, the resulting context batches should correspond to course concepts. Ideally, this ensures that the generated queries cover the full scope of the course material.

The resulting synthetic queries are then labeled by their origin course name, split into train, validation and test sets for which  each synthetic query is tokenized and each label is one-hot encoded respectively. 
These splits are stored are then stored on University of Oregon's HPC cluster, Talapas. 


### Methods Overview
- Detailed BackBone methods: [BackBone](./Models/BackBone/back_bone.md)
- Detailed DactBert methods: [DactBert](./Models/BackBone/back_bone.md)
- Detailed DactBert TrainingEngine: [TrainingEngine](./Models/DactBert/training_engine.md)
- Detailed DactBert TuningEnginge: [TuningEngine](./Models/DactBert/tuning_engine.md)

### Results
The model is currently running it's tuning and training pipeline, stay tuned. 

<<<<<<< HEAD
=======
# MoE-Gate
MoE-Gating for RAG:  infers routing decisions based on agentically generated training data via clustering expert knowledge embeddings.
=======
>>>>>>> f6a7621 (Update README.md)
