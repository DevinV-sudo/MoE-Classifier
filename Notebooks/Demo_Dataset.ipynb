{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8a3795-879b-4aa5-8720-a5e3e2baac6e",
   "metadata": {},
   "source": [
    "# Demo Notebook:\n",
    "\n",
    "This note book is a demo of the data-loader functionality. The Data-loader class code is in the 'Dataset' Directory, the data generation processes are located in the 'DataGen' directory if you wish to take a look at how the data being prepared in these loaders originated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c3b3041-ac10-4697-a117-4cdf77a1a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#transformer imports\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "#misc. imports\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "\n",
    "#hiding the warning that comes from torch vision\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "\n",
    "#set up logging config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7a8a7-6102-42fa-8932-69c1d5bd4a5f",
   "metadata": {},
   "source": [
    "### Dataloader Demo\n",
    "In the following to cells I import the create_dataloaders function from the Dataset directory and use it generate the dataloaders for each datasplit.\n",
    "To do so, I temporarily move the directorys to the path such that the modules can be imported for use in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8216fb46-dafb-41f9-83bf-513abb9bc241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 20:59:41,888 - INFO - PyTorch version 2.4.1 available.\n"
     ]
    }
   ],
   "source": [
    "# Add project root to sys.path\n",
    "script_dir = os.path.dirname(os.path.abspath(\"__main__\"))\n",
    "project_root = os.path.dirname(script_dir)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "#data loader imports\n",
    "from Dataset.data_loaders import create_dataloaders\n",
    "\n",
    "#model method imports\n",
    "from ModelMethods.dact_bert_methods import dact_bert_training_loop, dact_bert_plotting\n",
    "from ModelMethods.methods import train_step, evaluation_step\n",
    "\n",
    "#import dact-bert\n",
    "from Models.DACT_BERT import DACT_BERT\n",
    "from Models.VanillaBert import VanillaBert\n",
    "from Models.DactBert.dact_bert import TrainingEngine, DactBert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79faef-d3a0-4405-9923-148c593db23c",
   "metadata": {},
   "source": [
    "### DataLoader Functions:\n",
    "---\n",
    "The Dataloader \"create_dataloaders\" function takes two parameters: Batch_size and Split_prop. The split prop is an option of dataset split proportions, it is derived from the way that the datasets are stored on \"Talapas\", four datasets were generated each with a different train, test, and validation split. Those options are: 0.1, 0.2, 0.3, 0.4, any other option inputed will result in an error, unless you go and run another data-generation round with your desired split.\n",
    "For this demo, I will use 0.3 as the selected split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77fc850-88f1-4286-b823-8ce9053a47d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 20:59:43,848 - INFO - Loading tokenizer...\n",
      "\n",
      "2025-03-14 20:59:44,107 - INFO - Successfully loaded tokenizer.\n",
      "\n",
      "2025-03-14 20:59:44,108 - INFO - Loading Datasets...\n",
      "\n",
      "2025-03-14 20:59:44,111 - INFO - Checking Split Choice: Train...\n",
      "2025-03-14 20:59:44,112 - INFO - Selected valid split.\n",
      "\n",
      "2025-03-14 20:59:44,113 - INFO - Attempting to load data\n",
      "\n",
      "2025-03-14 20:59:44,135 - INFO - Successfully Loaded Dataset.\n",
      "\n",
      "2025-03-14 20:59:44,135 - INFO - Checking Split Choice: Valid...\n",
      "2025-03-14 20:59:44,136 - INFO - Selected valid split.\n",
      "\n",
      "2025-03-14 20:59:44,138 - INFO - Attempting to load data\n",
      "\n",
      "2025-03-14 20:59:44,163 - INFO - Successfully Loaded Dataset.\n",
      "\n",
      "2025-03-14 20:59:44,163 - INFO - Checking Split Choice: Test...\n",
      "2025-03-14 20:59:44,164 - INFO - Selected valid split.\n",
      "\n",
      "2025-03-14 20:59:44,166 - INFO - Attempting to load data\n",
      "\n",
      "2025-03-14 20:59:44,179 - INFO - Successfully Loaded Dataset.\n",
      "\n",
      "2025-03-14 20:59:44,179 - INFO - Successfully loaded datasets.\n",
      "\n",
      "2025-03-14 20:59:44,180 - INFO - Creating data loaders...\n",
      "\n",
      "2025-03-14 20:59:44,181 - INFO - Successfully created data loaders.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generate dataloaders options, are batch size and split proportion\n",
    "train_dataloader, valid_dataloader, test_dataloader = create_dataloaders(batch_size=256, split_prop=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3344363d-74a8-46cb-a564-9b4674825f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataloader Number of Batches: 3591\n",
      "\n",
      "Validation Dataloader Number of Batches: 1540\n",
      "\n",
      "Test Dataloader Number of Batches: 2199\n",
      "\n",
      "Dataloader Batch Contents: dict_items([('input_ids', tensor([[ 101, 2129, 2079,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2535,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2129, 2079,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 2129, 2079,  ...,    0,    0,    0]])), ('attention_mask', tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])), ('labels', tensor([1, 2, 0, 2, 2, 1, 1, 0, 0, 2, 2, 2, 1, 2, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0,\n",
      "        1, 1, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 1,\n",
      "        1, 2, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 0, 1, 2, 2, 2, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 2, 2, 0, 1, 2, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 1,\n",
      "        2, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 1, 0, 2, 2, 1, 1, 2, 0,\n",
      "        2, 0, 0, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 1, 2, 0, 0, 1, 2, 0, 1, 0, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 2, 0, 1, 2, 0, 1, 0, 1, 2, 0, 2, 0, 0, 1, 2,\n",
      "        1, 1, 0, 1, 1, 0, 1, 2, 1, 0, 0, 2, 1, 2, 1, 0, 1, 1, 2, 0, 2, 2, 1, 0,\n",
      "        2, 1, 1, 0, 1, 0, 2, 0, 2, 1, 0, 0, 1, 2, 2, 2, 2, 0, 1, 0, 2, 0, 1, 2,\n",
      "        2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 1, 1, 2, 1]))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the contents of the Dataloaders\n",
    "print(f\"Training Dataloader Number of Batches: {len(train_dataloader.dataset)}\\n\")\n",
    "print(f\"Validation Dataloader Number of Batches: {len(valid_dataloader.dataset)}\\n\")\n",
    "print(f\"Test Dataloader Number of Batches: {len(test_dataloader.dataset)}\\n\")\n",
    "\n",
    "print(f\"Dataloader Batch Contents: {next(iter(train_dataloader)).items()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
