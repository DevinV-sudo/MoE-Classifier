# Data Description:
Each of the dataloaders that are being generated via 'data_loaders.py' correspond to a HuggingFace dataset that exists at the default base path.
These datasets were collectively created from three seperate sources:

- DSCI 101 Fall 2025 course materials
- DSCI 410 Top Vis Winter 2025 course materials
- DSCI 410 Deep Learning Winter 2025 course materials  

The course materials contained a variety of unstructured multi-modal data, from textbooks to lecture recordings. This data was processed by a system I built
that can be viewed (in it's current working state) here: https://github.com/DevinV-sudo/chatO

# Preprocessing:

This dataset was created using the text embeddings generated by that system, using nomic's text embedding model. The text embeddings were stored in a vector index corresponding to their source course name. Each of the vector indices were clustered using PCA and HDBSCAN, and each cluster was then inverse transformed and the associated text for each component in each cluster was extracted. This text was then passed as context to GPT-3.5-Turbo to generate numbered lists of synthetic questions that represent possible queries a student in each course could ask about that "context" or course material. This data was then stored as a csv file which you can see in the following directory: 'DataGen/data/question_data/QuestionData.csv"

The question data is then split into train, valid, and test sets which live in the "Datagen/data/data_splits" directory. The "Question" feature of each split was then tokenized via the distil-bert-tokenizer and the "Label" feature was one-hot encoded.

These preprocessing tasks can be viewed in the 'DataGen' directory in this repo.

# Datasets

The resulting datasets are HuggingFace datasets, which consist of two features: 
- "Question": tokenized synthetic queries
- "Label": encoded course names

These are the datasets that are being loaded in "data_loaders.py"

# Important Notes

The methods used to generate the data are all available and documented in the DataGen Directory. However if you wish to run them for yourself you will need the following:

- OpenAI API Key
- PineCone API Key
- PineCone Index containing semantic context vectors, with the original embedding text as meta_data

I saved the QuestionData.csv to the Repo. if you wish to create and run your own version of the model, run process_data and just change the save dir endpoint in main to your local storage.


